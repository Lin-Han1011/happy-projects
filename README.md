# Lin Han's Individual Python Projects
The repository contains my personal predictive modeling and data cleaning and data visualization projects. 

1. The 'Bank client churn project' aims to forecast/classify if a client will leave the bank or not for an European bank. 
The dataset I used in my project is downloaded from Kaggle website: https://www.kaggle.com/adammaus/predicting-churn-for-bank-customers. It contains 10,000 observations and 14 variables, including 13 independent variables and 1 binary dependent variable 'Exited' that records if a bank client churn or not churn. 
I firstly explored the dataset and visualized variables and correlations to check if the assumptions of logistic regression model are met. Then, I used variables to build a logistic regression model and the final accuracy is 81.5%. I also developed two random forest non-parameter models using different variables (the higher testing accuracy is 86.85%) to compare with the logistic regression model, and further improvement will be made for both models. 

2. The 'Data preprocessing capital budget Toronto project' aims to clean the dirty data, and then build a Tableau dashboard (link: https://public.tableau.com/profile/lin.han8837#!/) using Toronto open source data set "Budget - Capital Budget & Plan By Ward (10 Year Recommended)".The data set source is https://www.toronto.ca/city-government/data-research-maps/open-data/open-data-catalogue/finance/#463113ed-6ad1-c05f-9ed5-f8965f40f7d3. 
The topic is "Future 5 years opportunities of city construction projects in Toronto". The potential audiences are "Representatives of Construction firms". My dashboard can provide insights of what is the 10-year (from 2015 to 2024) trend of governments’ investment on Toronto Construction projects, especially the budget allocation for next 5 years. Therefore, they can do preparations in advance and increase their competitive advantages such as buying equipment or hiring employees.

3. The 'MySQL Database Information Extraction Project' aims to extract and process tables stored in MySQL using SQLAlchemy and Pandas packages. 
I firstly imported packages and connected to a MySQL relational database named "sql_invocing" which stored 4 tables: 'clients', 'invoices', 'payment_methods', 'payments'. Then, I checked tables' details in the database and printed all table's primary key column and other columns. Thirdly, I created Pandas dataframes from MySQL tables and set index to the corresponding primary key. Lastly, I built analytical SQL queries to (1)extract subsets of information, (2)update clients' payment information, (3)create new clients' records, (4)delete useless record.

4. The "COVID-19 Fatal Probability Calculator Project" is to utilize data solution to reveal deeper reasons behind COVID-19 fatal cases in Toronto. I created this model when I attended the Hack Instead 48 - hour event (August 8th to 9th, 2020). You may also have a look of my presentation link posted in YouTube: https://www.youtube.com/watch?v=ZluhaOT8XgE&feature=youtu.be
The dataset was downloaded from Toronto Open Data 'COVID-19 CASES IN TORONTO' : https://open.toronto.ca/dataset/covid 19 cases in toronto/. 

5. The "Retail Store Performance Analytics Project" aims to reveal relationships between 45 Store Sales performance and Holiday Mark Downd frequenct. The dataset I chose is “Retail Data Analytics-Historical sales data from 45 stores” downloaded from Kaggle website (https://www.kaggle.com/manjeetsingh/retaildataset?select=stores+data-set.csv).
The dataset contains 3 Tables – Stores, Features and Sales. I conduct data exploration, data manipulation and hypthesis test of 2 variables ‘Holiday Week Numbers’ and ‘Mark Down to Sales Ratio’. They are positively correlated (0.75) and my test is significant based on p-value (2.33e-9). 
The correlation reveals the higher holiday week number, the higher Mark Down to Sales Ratio that means the profit ratio is lower. Therefore, retail company should pay more attention to the frequency of holiday mark down and avoid the decrease of profitability. 
Further Considerations are finding more correlations of store performance and input variables, comparing 45 stores' historical performance and building Machine Learning models to predict sales.
